{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_spatial_patterns_in_excavation_data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hiQg_M8G-BSd"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8JEjBNu-BQW",
        "colab_type": "text"
      },
      "source": [
        "# **More about spatial patterns**\n",
        "\n",
        "\n",
        "\n",
        "Understanding the meanings behind patterns of finds recovered through excavation is a tricky problem. We hope to distinguish activity areas, places devoted to domestic and industrial use, or inhabited places that are distinct from liminal ones. We often want to discern change over time, identifying areas with finds associated with different temporal periods. \n",
        "\n",
        "To successfully unravel these patterns, we must look not only at the **distributions** of different types of finds, but how they **correlate** with one another, the character of the contexts in which they were recovered, and their own physical and social characteristics. Are they likely to be curated? Are they light and likely to be moved from one area to another by post-depositional processes? It's all a bit of a mess. \n",
        "\n",
        "Importantly, all these processes are spatial. Alignments or proximity between areas with similar (or quite different) finds is potentially meaningful. \n",
        "\n",
        "The aims of this exercise is for you to:\n",
        "* learn to work real special finds data from an excavation, in all its messiness, to look for spatial patterns and relationships. **-> this is all about identifying better spatial patterns**\n",
        "* start thinking about quantitative and spatial approaches to finds data from excavations and how they can help us better understand the patterns we see. **-> this is all about quantifying better spatial patterns**\n",
        "\n",
        "You'll do this using data collected by the Gabii Project, a 10+ year excavation in central Italy. \n",
        "\n",
        " \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqgoptAiXijR",
        "colab_type": "text"
      },
      "source": [
        "### Let's get started...####  \n",
        "<font color='orangered'> ~ déjà vu ~</font> \n",
        "* Make your own copy of this notebook;\n",
        "* Get your tools... it is like for writing on paper, you need a pen, here you have to import your libraries...\n",
        "* Remember to hit play or type 'Ctrl'+'Enter' to run the code in any cell (grey shaded cells in the page) to make things happen!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awel6EMM-BQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ImportUrLibraries\n",
        "\n",
        "# as usual, start by getting your tools: your prerequisites.\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "# Matplotlib is your tool for drawing graphs and basic maps. You need this!\n",
        "!pip install fiona\n",
        "!pip install geopandas\n",
        "import pandas as pd\n",
        "import requests\n",
        "import fiona\n",
        "import geopandas as gpd\n",
        "import ipywidgets as widgets\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpZN9KC_ZxJ5",
        "colab_type": "text"
      },
      "source": [
        " **Learning a new language – decomposing the code** \n",
        "  <br>\n",
        "  in #codecell_SpatialPatterns_ImportUrLibraries. These are what we call **prerequisites**. You know by now that they are basic tools so you can get started.\n",
        "* *Pandas* manipulate data. \n",
        "* *Geo-pandas* manipulate geographic data. They're also black and white and like to eat bamboo...  You need these to manipulate your data!\n",
        "* *Fiona* helps with geographic data (find more about [fiona](https://pypi.org/project/Fiona/)).\n",
        "* *Requests* are for asking for things. It's good to be able to ask for things.\n",
        "* *ipywidgets* supports interactivity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDGLKQ8_iwBc",
        "colab_type": "text"
      },
      "source": [
        "### **Getting to know your Data...**####  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siGQqpwn-BQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ImportUrData\n",
        "\n",
        "# then get your data\n",
        "# This is where I put the data. It's in a format called geojson, used to represent spatial geometry (shapes) and attributes (text).\n",
        "url = 'http://ropitz.github.io/digitalantiquity/data/gabii_SU.geojson'\n",
        "\n",
        "# Please get me the data at that web address (url):\n",
        "# use requests.get to retrieve data from any destination\n",
        "request = requests.get(url)\n",
        "\n",
        "# I will use the letter 'b' to refer to the data, like a nickname\n",
        "#we can use requests to read the response content in bytes\n",
        "b = bytes(request.content)\n",
        "\n",
        "#So we will use fiona.BytesCollection referred by the letter 'f':\n",
        "# to read the raw data (as single-file formats or zipped shapefiles)\n",
        "# to wrap up all the data from 'b'\n",
        "# check the coordinate refereence system (crs) listed in the features\n",
        "with fiona.BytesCollection(b) as f:\n",
        "    crs = f.crs\n",
        "    #by using also GeoDataFrame.from_features you can read geospatial data that's in the url without saving that data to disk (your PC) first\n",
        "    gabii_su_poly = gpd.GeoDataFrame.from_features(f, crs=crs)\n",
        "    # and print out the first few lines of the file, so I can check everything looks ok: you know this by now...we will call .head()\n",
        "    print(gabii_su_poly.head())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bXO2Veyf4b-",
        "colab_type": "text"
      },
      "source": [
        "**Learning a new language – decomposing the code** \n",
        "  <br>\n",
        "  in #codecell_SpatialPatterns_ImportUrData:<br>\n",
        " \n",
        " <font color='magenta'>  *GeoJSON*  </font>  is used to store the excavation data. GeoJSON format allows to encode a variety of geographic data structures which contains features with spatial attributes (e.g. points, line strings, polygons, multiparts geometries) and non-spatial attributes (text). This is a really useful format to use when creating a GIS. <br>\n",
        "  <font color='magenta'>  *bytes()*  </font>  method returns bytes object which is an immmutable (cannot be modified) sequence of integers. We tend to use this to compress data, save or send it. <br>\n",
        "  <font color='magenta'>  *requests.get*  </font> is used to retrieve data from any destination & <font color='magenta'>  *requests.content*  </font>  to read all content which is in bytes (for non-text requests using the .content property).<br>\n",
        "<font color='magenta'>  *BytesCollection()*  </font>  takes a buffer of bytes and maps to a virtual file that can then be opened by fiona. By using  both fiona.BytesCollection and GeoDataFrame.from_features you can:\n",
        "* to read the raw data (as single-file formats or zipped shapefiles)\n",
        "* to wrap up all the data from 'b'\n",
        "* check the coordinate refereence system (crs) listed in the features\n",
        "\n",
        "\n",
        " **Open source**\n",
        "\n",
        "For the past month, we have abundantly benefitted from open-source software, data, tools, code, design documents, or content. It is only natural to open, share and use the results of 10 years excavation..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aXd2B1bre1_",
        "colab_type": "text"
      },
      "source": [
        "### **Assessing visually your Data...**####  \n",
        "   So you know what's in this dataset... Maybe you want to see it before you can question it? ... Start by visualising the spatial data for all the contexts (stratigraphic units) from the excavation we'll be exploring.\n",
        "\n",
        "So far you have dealt with survey data where all data has been logged with coordinates (x, y and sometimes z for the elevation height). However, it is not always possible, or even meaningful, to record the coordinates of all artefacts retrieved during an excavation, especially if it lasts over several years. It is then more appropriate to use stratigraphical units (SU). The spatial analysis of these finds is more subtle as they don't have a spatial location per se. We need to think carefully about organising and presenting them. AND colour can be a great help for this!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIYnLr0-BQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_PlottingUrData\n",
        "\n",
        "\n",
        "# Now we have polygons, the shapes of our contexts. Let's visualise the data to double check that all is well\n",
        "# We'll use again the function .plot (see lab_Webmaps&Distributions)\n",
        "# 'plot' means draw me an image showing the geometry of each feature in my data. \n",
        "# We want to control things like the color of different types of features on our map. \n",
        "# I used the 'Blues' colorscale command (cmap stands for 'colour map') \n",
        "# and asked it to draw the polygons differently based on the type of feature.\n",
        "\n",
        "gabii_map1 = gabii_su_poly.plot(column='DESCRIPTIO', cmap='Blues', edgecolor='grey', figsize=(15, 15));\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVgg9fN1-BQj",
        "colab_type": "text"
      },
      "source": [
        "####**Learning a new language – decomposing the code** ####\n",
        "\n",
        "In #codecell_SpatialPatterns_PlottingUrData, when using <font color='magenta'>  .plot()</font>, when <font color='magenta'>  column=' '  </font>  is specifed,  the plot colouring is based on its values. The colour scale is defined by using <font color='magenta'>  cmap=''   </font> , the edge of the features (our polygons) are defined by <font color='magenta'>  edgecolor=''   </font>  and the size of the plot by <font color='magenta'>  figsize=(width, height))   </font> .\n",
        "\n",
        "you can of course add parameters/symbologies to your plots. The choice of parameters is largely dictated by your data analysis. [Here](http://geopandas.org/mapping.html) is some documentation on plots generation.\n",
        "\n",
        "The colorscale options are: Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r, afmhot, afmhot_r, autumn, autumn_r, binary, binary_r, bone, bone_r, brg, brg_r, bwr, bwr_r, cividis, cividis_r, cool, cool_r, coolwarm, coolwarm_r, copper, copper_r, cubehelix, cubehelix_r, flag, flag_r, gist_earth, gist_earth_r, gist_gray, gist_gray_r, gist_heat, gist_heat_r, gist_ncar, gist_ncar_r, gist_rainbow, gist_rainbow_r, gist_stern, gist_stern_r, gist_yarg, gist_yarg_r, gnuplot, gnuplot2, gnuplot2_r, gnuplot_r, gray, gray_r, hot, hot_r, hsv, hsv_r, inferno, inferno_r, jet, jet_r, magma, magma_r, nipy_spectral, nipy_spectral_r, ocean, ocean_r, pink, pink_r, plasma, plasma_r, prism, prism_r, rainbow, rainbow_r, seismic, seismic_r, spring, spring_r, summer, summer_r, tab10, tab10_r, tab20, tab20_r, tab20b, tab20b_r, tab20c, tab20c_r, terrain, terrain_r, viridis, viridis_r, winter, winter_r\n",
        "\n",
        "Swap out 'Blues' in the cell above for any of these options...\n",
        "\n",
        "<img src=\"https://matplotlib.org/3.1.1/_images/sphx_glr_colormaps_002.png\" width=\"400\"/> </div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asEFZLAQr04v",
        "colab_type": "text"
      },
      "source": [
        "###**Loading the special finds**### \n",
        "\n",
        "Like many excavations, not every special find in this datset has spatial coordinates associated with it (because in real archaeology life things are found in the sieve, the wheelbarrow, and during washing). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zikTZlaj-BQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_WhichTypeOfSpecialFinds&fromWhere?\n",
        "\n",
        "# Now I'm going to bring in all the basic Gabii special finds data - descriptions, object types, IDs and the contexts from which they come.\n",
        "# We've had a few special finds over the years.\n",
        "sf_su = pd.read_csv(\"https://raw.githubusercontent.com/ropitz/gabii_experiments/master/spf_SU.csv\")\n",
        "sf_su"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUKJYdCLfGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_WhichTypeOfSpecialFinds?\n",
        "\n",
        "#the set()function allows to return all values (here our special finds type) without duplicates \n",
        "#this is a useful tool when you need to standardise your finds labels (and check your metadata for spelling!)\n",
        "sf_su_desc = sf_su['SF_OBJECT_TYPE']\n",
        "set(sf_su_desc)\n",
        "\n",
        "#however, this is a really long list to deal with, so we need to find ways to prepare this dataset to really see what has been happening on this site."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JPhVomY-BQn",
        "colab_type": "text"
      },
      "source": [
        "One of our area supervisors, Troy, is super excited about tools related to textile production. They're a great example of how we think about special finds at Gabii. Multiple types of finds are related to textile production. Do we find all types everywhere? Are certain types of tools more concentrated in one type of context or one area than others? Troy has lots of questions about the patterns of places where we find these tools. Do they provide evidence for early textile production? Are they a major factor in the city's early wealth? Do we find the same things in later periods? After all, people under the Republic and Empire wore clothes... Loom Weights, spools, and spindle whorls are the most common weaving tools at Gabii.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlxZSplR5W6",
        "colab_type": "text"
      },
      "source": [
        "###**Preparing your data, a prerequisite to all analysis**###\n",
        "\n",
        "#### **Selection**#### \n",
        "As this data is not yet spatial and only associated with a stratigraphic unit, logically, we can merge our non-spatial special finds data with our spatial stratigraphic units data to make all our data spatial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCrCdSIQ-BQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_SpecialFindsSelection\n",
        "\n",
        "#Let's pull all those find types out of the big list. \n",
        "#We're selecting the finds data we want to work with before merging with the spatial data. We could do these operations in reverse if we wanted to.\n",
        "#here very much like in lab1,#codecell_makeabasicmap_BringingUrData2theMap, & lab2, #codecell_Webmaps&Distributions_SplittingUrData, we are using iloc and isin functions\n",
        "\n",
        "types = ['Loom Weight','Spool','Spindle Whorl']\n",
        "textile_tools = sf_su.loc[sf_su['SF_OBJECT_TYPE'].isin(types)]\n",
        "textile_tools\n",
        "\n",
        "#we now have a new dataframe containing only textile_tools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChQKlGPcs9qM",
        "colab_type": "text"
      },
      "source": [
        "#### **Listing and merging to become spatial**#### \n",
        "Presence or absences isn't everything. You may want to know how many of a certain type of find is present in a given area."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QG9Hbhn0-BQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_TextileToolsListing\n",
        "\n",
        "# Now let's count up how many of these tools appear in each context (SU).\n",
        "# pd.value_counts() functioon returns a series containing counts of unique values.\n",
        "# So we can print out a list of the number of textile_tools in each SU next to that SU number.\n",
        "\n",
        "pd.value_counts(textile_tools['SU'].values, sort=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3x4foKL-BQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_TextileToolsBecomesSpatial\n",
        "\n",
        "#Then let's combine the special finds data with our polygons representing context with shape and a spatial location\n",
        "# We do this with a command called 'merge'. In lab2,#codecell_Webmaps&Distributions_MergingZeData, you have used pandas pd.merge()\n",
        "\n",
        "gabii_textools = gabii_su_poly.merge(textile_tools, on='SU')\n",
        "\n",
        "# very much like p.merge(), you have now created a new dataframe ('gabii_textools') by merging dataframe 'textile_tools' on= SU, the stratigraphical unit\n",
        "# let's have a look at the new dataframe using  .head() to print out just the first few rows.\n",
        "gabii_textools.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhKJFX1wXMkP",
        "colab_type": "text"
      },
      "source": [
        "#### **Visual assessment**#### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXxiPahi-BQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_SeeingTextileToolsinContext\n",
        "\n",
        "# If we want to see this result as a map, we just add the .plot command to the end of the dataframe's name\n",
        "# here .plot() symbology is expanded to transparency  with 'alpha=' where value of 1 is complete opacity and 0 complete transparency  \n",
        "\n",
        "gabii_textools.plot(column='SF_OBJECT_TYPE', cmap='Accent', figsize=(15, 15), legend=True, alpha=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87-PZM6l-BQz",
        "colab_type": "text"
      },
      "source": [
        "####**But what do you see really?**###\n",
        "OK, what do you see here? Compare the distribution of each type of textile tool. Do some types seem to be **concentrated** in certain areas? How might you check? What **factors** might contribute to this pattern? Do big layer simply aggregate lots of stuff? Do late dumps contain early materials? Why would one type of tool appear where the others don't?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY3cT_mV-BQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_SortingDataTextileTools\n",
        "\n",
        "# We can try and see the relationship between layer size and count by sorting\n",
        "#our list of finds by the surface area of each layer.\n",
        "# We use the command 'sort_values' \n",
        "\n",
        "gabii_textools.sort_values(by=['Shape_Area'],ascending=False)\n",
        "\n",
        "# '.sort_values' function sort along their axis (here the axis is defined by 'Shape_Area' ). \n",
        "# the default sorting is on ascending values (smallest to largest) if you are happy with this =True, however, we want to see them in descending order, so we select =False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQKRWndWJkqb",
        "colab_type": "text"
      },
      "source": [
        "####**Knowing your site and refining your analysis**####\n",
        "Gabii excavations have revealed that there are enormous colluvial layers. This is an important consideration as these large areas will contribute to a bias distribution of the artefacts across the site. Therefore, very large areas should probably be excluded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs3Lvg3O-BQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_RefiningDataSorting\n",
        "\n",
        "# Outliers will mess with any analysis. Here large stratigraphical layer are our outliers\n",
        "# By cutting out these layers i.e. excluding SUs with a surface area greater than 800 we can deal with these outliers\n",
        "\n",
        "gabii_textools2 = gabii_textools.loc[gabii_textools['Shape_Area']<800]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXgBTzkX-BQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_VisualisingDataSorting\n",
        "\n",
        "# If we want to see this result as a map, we just add the .plot command to the end again.\n",
        "\n",
        "gabii_textools2.plot(column='SF_OBJECT_TYPE', cmap='Accent', figsize=(15, 15), legend=True, alpha=0.5)\n",
        "\n",
        "# That's better. Plot the results to see that you've removed the big colluvial layers."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skqDgXvhNKgq",
        "colab_type": "text"
      },
      "source": [
        "####**Grouping and merging further**#### \n",
        "to answer to question: how many of each tool type appears in each SU? You will need to group and merge further your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW8PUkQI-BQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_GroupingData\n",
        "\n",
        "# OK, count up how many of each tool type appears in each SU using the 'groupby' command. \n",
        "# You have used this command before in #codecell_Webmaps&Distributions_SplittingUrData_CreateLayers \n",
        "## and .fillna() was explained in codecell_Webmaps&Distributions_AllNumbers \n",
        "\n",
        "textools_counts = gabii_textools2.groupby('SU')['SF_OBJECT_TYPE'].value_counts().unstack().fillna(0)\n",
        "\n",
        "\n",
        "# Sort the list so that the SUs with the most stuff end up at the top.\n",
        "textools_counts.sort_values(by=['Loom Weight','Spindle Whorl','Spool'], ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZsp42WE-BRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_MergingData\n",
        "\n",
        "# Merge your textile tool counts with your spatial data for the contexts\n",
        "# Because both dataframes have a 'SU' column, you can use this to match up the rows. \n",
        "# so the merger will occur on='SU'\n",
        "\n",
        "gabii_textools_counts = gabii_su_poly.merge(textools_counts, on='SU')\n",
        "gabii_textools_counts.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQVlzdgZtU8c",
        "colab_type": "text"
      },
      "source": [
        "#### **Visual assessment: exploring your data**#### \n",
        " Side by side plots of different variables  can help you to visualize the differences between the spatial patterns you're exploring. Very much like in  lab2_MakeaBasicMap, when you compared Late Roman and\n",
        "Middle Roman artefact distributions using two heatmaps side-by-side. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_Wmixp-BRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_AssessingFindType\n",
        "\n",
        "# Let's start by looking at each class of textile tool individually. \n",
        "# Plot the counts of each type of find spatially\n",
        "\n",
        "gabii_textools_counts.plot(column='Loom Weight', cmap='Accent', figsize=(15, 15), legend=True, alpha=0.5, legend_kwds={'label': \"Number of Loom weight\",'orientation': \"vertical\"})\n",
        "gabii_textools_counts.plot(column='Spindle Whorl', cmap='Accent', figsize=(15, 15), legend=True, alpha=0.5, legend_kwds={'label': \"Number of Spindle Whorl\",'orientation': \"vertical\"})\n",
        "gabii_textools_counts.plot(column='Spool', cmap='Accent', figsize=(15, 15), legend=True, alpha=0.5, legend_kwds={'label': \"Number of Spool\",'orientation': \"vertical\"})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBDAmDtW-BRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_PlottingAllFindType\n",
        "\n",
        "base = gabii_textools_counts.plot(column='Loom Weight', cmap='Blues', figsize=(15, 15), alpha=0.7)\n",
        "gabii_textools_counts.plot(ax=base, column='Spindle Whorl', cmap='Reds', alpha=0.7)\n",
        "gabii_textools_counts.plot(ax=base, column='Spool', cmap='Greens', alpha=0.7);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOFt2mulEpQ",
        "colab_type": "text"
      },
      "source": [
        "**Let's get another libraries to help visualisation**\n",
        "So far, it has been difficult to see what's happening, to identify activities between the buildings and to compare the maps when we have to scroll.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPhlEUHClPK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ImportUrLibraries\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qjbSNcx-BRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###codecell_SpatialPatterns_AllFindTypeSidebySide\n",
        "\n",
        "# Let's put the maps side by side.\n",
        "fig, axes = plt.subplots(ncols=3,figsize=(15, 5))\n",
        "gabii_textools_counts.plot(column='Loom Weight', cmap='autumn',  ax=axes[0], legend=True, legend_kwds={'label': \"Number of Loom weight\",'orientation': \"vertical\"}).axis('equal')\n",
        "gabii_textools_counts.plot(column='Spindle Whorl', cmap='autumn', ax=axes[1], legend=True, legend_kwds={'label': \"Number of Spindle Whorl\",'orientation': \"vertical\"}).axis('equal')\n",
        "gabii_textools_counts.plot(column='Spool', cmap='autumn',ax=axes[2], legend=True, legend_kwds={'label': \"Number of Spool\",'orientation': \"vertical\"}).axis('equal')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82KrdqwQ-BRM",
        "colab_type": "text"
      },
      "source": [
        "###**Questionning your maps**###\n",
        "Can you see any **patterns** here? Do the different types of tools **concentrate** in the same parts of the site? Why might different types of tools have different **distributions**? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_WBqeTEtcPh",
        "colab_type": "text"
      },
      "source": [
        "### ~ ###\n",
        "*OK, this next big scary cell is because google has broken something in colab after I drafted this exercise. Push run to fix the thing they've broken (hopefully).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l6FaKTG92ZJ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "\n",
        "#@title\n",
        "!apt-get install -qq curl g++ make\n",
        "#@title\n",
        "!curl -L http://download.osgeo.org/libspatialindex/spatialindex-src-1.8.5.tar.gz | tar xz\n",
        "#@title\n",
        "import os\n",
        "os.chdir('spatialindex-src-1.8.5')\n",
        "#@title\n",
        "!./configure\n",
        "#@title\n",
        "!make\n",
        "#@title\n",
        "!make install\n",
        "#@title\n",
        "!pip install rtree\n",
        "#@title\n",
        "!ldconfig\n",
        "#Working through the example at http://toblerity.org/rtree/examples.html\n",
        "#@title\n",
        "from rtree import index\n",
        "from rtree.index import Rtree\n",
        "#@title\n",
        "p = index.Property()\n",
        "idx = index.Index(properties=p)\n",
        "idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeUS-o2Jm6dW",
        "colab_type": "text"
      },
      "source": [
        "###**Quantifying these patterns**###\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVtbdZPW-BRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ImportUrLibraries\n",
        "\n",
        "# I think the distributions of different weaving tools vary.\n",
        "# To investigate further, we are going to need more tools.\n",
        "!pip install pysal\n",
        "import pysal\n",
        "from sklearn import cluster\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qHclmig-BRP",
        "colab_type": "text"
      },
      "source": [
        "####**Data Clustering**#### \n",
        "We're going to use **cluster analysis** to try and better understand our patterns. Clustering is a broad set of techniques for finding groups within a data set. Cluster analysis has as its objective the grouping together of similar observations (unlike factor analysis works by searching for similar variables).\n",
        "<br> \n",
        "Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. It is very much a statistical problem where the  algorithm tests hypothesis that data points of the same group have similar properties/features against/versus data points in different groups have dissimilar properties/features. So, when we cluster observations, we want items in the same group to be similar and items in different groups to be dissimilar. <br>  \n",
        "**K-Means clustering** is probably the most well-known clustering algorithm that  solve  clustering problem by splitting  a dataset into a set of k (k being an arbitrary number you get to choose) groups. \n",
        " <br> K-Means Clustering explained in 5 visual steps:\n",
        "* Step1: assign each points to similar centre => we need to identify the number of classes to use by  looking at the data and identifying any discrete groupings.  <br> \n",
        " <img src=\"https://github.com/Francoz-Charlotte/Spatial_teaching_CFediting/blob/master/LAb4_patternss_clusters.png?raw=1\" width=\"200\"/> </div>  <br> \n",
        "* Step2: identify the cluster centroids (3 coloured symbols on graph). <br> \n",
        "<img src=\"https://developers.google.com/machine-learning/clustering/images/Initialization.svg\" width=\"200\"/> </div>  <br> \n",
        "* Step 3: reassign the points based on the minimum distance or closest from cluster centroids. Here we should emphasise that there many possible definitions that may be used for “closest” (e.g.nearest neighbour, Ward's method).   <br> \n",
        "<img src=\"https://developers.google.com/machine-learning/clustering/images/Step2.svg\" width=\"200\"/> </div>  <br> \n",
        "* Step 4: identify the new centroids by taking the average of all points in the cluster.  <br> \n",
        "<img src=\"https://developers.google.com/machine-learning/clustering/images/Step3.svg\" width=\"200\"/> </div>  <br> \n",
        "* Step 5: reassign the groupings -points and assignment- until points stop changing clusters in a loop. <br> \n",
        "<img src=\"https://developers.google.com/machine-learning/clustering/images/Step4.svg\" width=\"200\"/> </div>  <br> \n",
        "\n",
        "\n",
        "Because clustering allows us to identify which things are alike on the basis of multiple characteristics, we will do just that, characterising our finds to interpret our dataset and answer some of the questions posed our mapping. In #codecell_SpatialPatterns_ImportUrLibraries, we have now imported  cluster from sklearn to help us with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aImzRH4J3Xdp",
        "colab_type": "text"
      },
      "source": [
        "###**Clustering types of textile tools**###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk1ZjHSFoHkK",
        "colab_type": "text"
      },
      "source": [
        "####**Let's use python to run K-means function**####\n",
        "First, we want to cluster together contexts where the pattern of the three types of textile tools are similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkkAU1rK-BRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ClusterTextileTools\n",
        "\n",
        "# Next step: cluster together contexts where the pattern of the three types of textile tools are similar, \n",
        "# with and without respect to the size of the context.\n",
        "# we will use three functions .cluster.Kmeans, .fit() and .drop():\n",
        "\n",
        "km5 = cluster.KMeans(n_clusters=5)\n",
        "km5cls = km5.fit(gabii_textools_counts.drop(['geometry', 'OBJECTID','DESCRIPTIO','Shape_Length','SU'], axis=1).values)\n",
        "km5cls\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34muNG5z6UGu",
        "colab_type": "text"
      },
      "source": [
        "#####**Learning a new language – decomposing the code** #####\n",
        "\n",
        "In #codecell_SpatialPatterns_ClusterTextileTools, by:\n",
        "* using <font color='magenta'>  .cluster.Kmeans()</font> function,\n",
        " we have arbitrarily given k (n_clusters) an arbitrary value of 5 groups\n",
        " <br> \n",
        "* using <font color='magenta'>  .fit()</font> function, we wanted to estimate the best representative function for the data points on account for the size of the context (*'Shape_Area'*) and counts of different types of tools(*Loom Weight, Spindle Whorl, Spool*). \n",
        "* using <font color='magenta'>  .drop()</font> function Drop all the other fields (*'geometry', 'OBJECTID','DESCRIPTIO','Shape_Length','SU'*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqmxImFv-BRW",
        "colab_type": "text"
      },
      "source": [
        "#####**let's visualise this first step**#####\n",
        "Each cluster produced should contain the SUs that are similar to one another on the basis of the number of each type of textile tool and the size of the surface area of the SU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZibZbvtQ-BRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_SubPlotClusterTextileTools\n",
        "\n",
        "# Plot the clusters, groups of contexts that have similar textile tool assemblages.\n",
        "# Give a different colour to the SUs that belong to each cluster.\n",
        "\n",
        "#creating a single chart\n",
        "f1, ax = plt.subplots(1, figsize=(15,15))\n",
        "#plt.subplots() is a function that returns a tuple (=sequence of immutable Python objects like a list)\n",
        "#this tuple contains a figure and axes object(s)\n",
        "#so, when using fig, ax = plt.subplots() you unpack this tuple into the variables fig and ax.\n",
        "#here f1 is your figure\n",
        "\n",
        "#here we want to assign() our new clustering labels 'km5cls.labels_'to the dataframe\n",
        "#and also plot the cluster ('cl'). (nb: that ax=ax are the features/objects here)\n",
        "gabii_textools_counts.assign(cl=km5cls.labels_)\\\n",
        "   .plot(column='cl', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=ax)\n",
        "\n",
        "ax.set_axis_off()\n",
        "#turn x and y-axis off which will affect the axis lines, ticks, ticklabels, grid and axis labels\n",
        "\n",
        "#let's see the plot using command plt.show()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cee0MaMR-BRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ClusterTextileTools_2\n",
        "\n",
        "#Do the same as in ##codecell_SpatialPatterns_ClusterTextileTools\n",
        "#but let's ignore the size of the context so we also .drop('Shape_Area').\n",
        "\n",
        "\n",
        "km5 = cluster.KMeans(n_clusters=5)\n",
        "km5cls2 = km5.fit(gabii_textools_counts.drop(['geometry', 'OBJECTID','DESCRIPTIO','Shape_Length','SU','Shape_Area'], axis=1).values)\n",
        "\n",
        "#we plot as in ##codecell_SpatialPatterns_SubPlotClusterTextileTools\n",
        "#this time a'f2' and our new cluster 'cl2'\n",
        "\n",
        "f2, ax = plt.subplots(1, figsize=(15,15))\n",
        "\n",
        "gabii_textools_counts.assign(cl2=km5cls2.labels_)\\\n",
        "   .plot(column='cl2', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=ax)\n",
        "\n",
        "ax.set_axis_off()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGObkUxd-BRf",
        "colab_type": "text"
      },
      "source": [
        "###**Start interpreting**###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUiX07wuhHnw",
        "colab_type": "text"
      },
      "source": [
        "####**Visualising clustering analysis differences**####\n",
        "The patterns are definitely different. How can we interpret the fact that context size affects the pattern of the distribution of textile tools? Do big units, which perhaps represent dumps or colluvial mashups, have a fundamentally different character than the varied small contexts?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9dmn2j4-BRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_SubPlotClustersSidebySide\n",
        "\n",
        "# Look at the difference with and without context size taken into account.\n",
        "\n",
        "#we are plotting just like in##codecell_SpatialPatterns_SubPlotClusterTextileTools\n",
        "#however we want to see them side by side to see effect of our selection.\n",
        "#To do so, we define our plt.subplots as having 2 columns (ncols=2)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(15, 5))\n",
        "\n",
        "\n",
        "gabii_textools_counts.assign(cl2=km5cls2.labels_)\\\n",
        "   .plot(column='cl2', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=axes[0]).axis('equal')\n",
        "gabii_textools_counts.assign(cl=km5cls.labels_)\\\n",
        "   .plot(column='cl', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=axes[1]).axis('equal')\n",
        "\n",
        "# note here that we have to 2 axes for our to feature classes: \n",
        "# ax=axes[0] for the first subplot and  ax=axes[1] for the second subplot "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhrydzJshzPs",
        "colab_type": "text"
      },
      "source": [
        "####**Characterising our cluster analyses**####\n",
        "There are many ways to label your data in matplolib, [here](https://nikkimarinsek.com/blog/7-ways-to-label-a-cluster-plot-python) are some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zevk9cjl-BRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_AddLabelClusterstoDataframe\n",
        "\n",
        "# assign the cluster IDs to each context permanently\n",
        "gabiitextools_clas = gabii_textools_counts.assign(cl=km5cls.labels_)\n",
        "gabiitextools_class = gabiitextools_clas.assign(cl2=km5cls2.labels_)\n",
        "\n",
        "#and let's check how your dataframe looks like\n",
        "gabiitextools_class.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOKka5YAjLWl",
        "colab_type": "text"
      },
      "source": [
        "####**Data interpretation: push further your cluster analyses**####\n",
        "I tend to call this playing with your data... this can be labour intensive but it helps to understand how well the cluster analysis performed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J5gcA1Uz7Yg",
        "colab_type": "text"
      },
      "source": [
        "#####*Let's compare the effect of area size to one cluster group/class*#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AIxnf2H-BRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_SubselectClustergroupN.0\n",
        "\n",
        "# Now let's look at some individual classes, with and without context size accounted for in the analyses.\n",
        "# We choose the cluster group/class 0\n",
        "# we use .loc[] command (see##codecell_makeabasicmap_BringingUrData2theMap &  #codecell__Webmaps&Distributions_SplittingUrData)\n",
        "# and create 2 dataframes:\n",
        "# With=> [gabiitextools_class['cl']where we only select (==) group 0\n",
        "# Without=> [gabiitextools_class['cl2']where we only select (==) group 0\n",
        "\n",
        "gabiitextools_class0=gabiitextools_class.loc[gabiitextools_class['cl']==0]\n",
        "gabiitextools_class0noarea=gabiitextools_class.loc[gabiitextools_class['cl2']==0]\n",
        "\n",
        "#we are plotting just like in##codecell_SpatialPatterns_SubPlotClustersSidebySide\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(15, 5))\n",
        "gabiitextools_class0.plot(ax=axes[0], legend=True).axis('equal')\n",
        "gabiitextools_class0noarea.plot(ax=axes[1]).axis('equal')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Lh1gTWMmwo7",
        "colab_type": "text"
      },
      "source": [
        "#####*Effect of changing cluster numbers*####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdCiHX2N-BRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ClusterTextileTools_7types_WithContextSize \n",
        "\n",
        "# What happens when we change the number of clusters (groups)?\n",
        "# let's repeat our first analysis (##codecell_SpatialPatterns_ClusterTextileTools) \n",
        "# and let's keep the context size ('Shape_area')\n",
        "km7 = cluster.KMeans(n_clusters=7)\n",
        "km7cls3 = km7.fit(gabii_textools_counts.drop(['geometry', 'OBJECTID','DESCRIPTIO','Shape_Length','SU'], axis=1).values)\n",
        "f3, ax = plt.subplots(1, figsize=(15,15))\n",
        "\n",
        "gabii_textools_counts.assign(cl3=km7cls3.labels_)\\\n",
        "   .plot(column='cl3', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=ax)\n",
        "\n",
        "ax.set_axis_off()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfWmcs0StUCo",
        "colab_type": "text"
      },
      "source": [
        "That also changes things. Without going into too much detail, finding the ideal number of clusters is a **black art**. Try playing around with the number of clusters in the notebook, or the size cut-off for inclusion. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwtRjnKo-BRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_ClusterTextileTools_7types_WithoutContextSize\n",
        "\n",
        "\n",
        "# Use 7 clusters folowing the same procedure than in ##codecell_SpatialPatterns_ClusterTextileTools_7types_WithContextSize \n",
        "# and let's drop the context size ('Shape_area')\n",
        "\n",
        "km7 = cluster.KMeans(n_clusters=7)\n",
        "km7cls4 = km7.fit(gabii_textools_counts.drop(['geometry', 'OBJECTID','DESCRIPTIO','Shape_Length','SU','Shape_Area'], axis=1).values)\n",
        "f4, ax = plt.subplots(1, figsize=(15,15))\n",
        "\n",
        "gabii_textools_counts.assign(cl4=km7cls4.labels_)\\\n",
        "   .plot(column='cl4', categorical=True, legend=True, \\\n",
        "         linewidth=0.1, cmap='Accent', edgecolor='white', ax=ax)\n",
        "\n",
        "ax.set_axis_off()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfkWHiOZ-BR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_AddLabelto7ClusterGroups\n",
        "\n",
        "# assign the cluster IDs to each context permanently just like in ##codecell_SpatialPatterns_AddLabelClusterstoDataframe\n",
        "\n",
        "# Let's set up to investigate some of the individual clusters\n",
        "gabiitextools_class3=gabiitextools_class.assign(cl3=km7cls3.labels_)\n",
        "gabiitextools_class4=gabiitextools_class3.assign(cl4=km7cls4.labels_)\n",
        "gabiitextools_class4.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6CSTrxDxGD3",
        "colab_type": "text"
      },
      "source": [
        "#####*Let's compare the effect of changing cluster number (from 5 to 7) and area size to one cluster group/class*#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8MuyIpe-BR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_EffectofClusterNumber&AreaSizetoClustergroupN.0\n",
        "\n",
        "#we are following the same step than in ##codecell_SpatialPatterns_SubselectClustergroupN.0 but with our 7 cluster groups/classes\n",
        "# set up variables to store several classes, with and without context size taken into account.\n",
        "gabiitextools_class0=gabiitextools_class4.loc[gabiitextools_class4['cl']==0]\n",
        "gabiitextools_class0noarea=gabiitextools_class4.loc[gabiitextools_class4['cl2']==0]\n",
        "gabiitextools_k7_class0=gabiitextools_class4.loc[gabiitextools_class4['cl3']==0]\n",
        "gabiitextools_k7_class0noarea=gabiitextools_class4.loc[gabiitextools_class4['cl4']==0]\n",
        "\n",
        "# setting the subplots to the four dataframes, we have just identified above\n",
        "# this can be done by arranging the subplots like a 2X2 table/array with 'ncols=2' & 'nrows=2'\n",
        "fig, axes = plt.subplots(ncols=2,nrows=2,figsize=(15, 10))\n",
        "\n",
        "gabiitextools_class0.plot(ax=axes[0,0]).axis('equal')\n",
        "#note here the ax=axes defines the position of the figure in this 2X2 array (in this case is cell column 0, row0)\n",
        "axes[0,0].set_title('cl - 5 clusters - area')\n",
        "#for ease, we have added a title to the subplots\n",
        "\n",
        "gabiitextools_class0noarea.plot(ax=axes[0,1]).axis('equal')\n",
        "axes[1,0].set_title('cl2 - 5 clusters - no area')\n",
        "gabiitextools_k7_class0.plot(ax=axes[1,0]).axis('equal')\n",
        "axes[0,1].set_title('cl3 - 7 clusters - area')\n",
        "gabiitextools_k7_class0noarea.plot(ax=axes[1,1]).axis('equal')\n",
        "axes[1,1].set_title('cl - 7 clusters - no area')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuUJafidw3Y_",
        "colab_type": "text"
      },
      "source": [
        "#####*Let's compare the effect of changing cluster number (from 5 to 7) and area size to other cluster groups/classes*#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoiyIy7o-BR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_EffectofClusterNumber&AreaSizetoClustergroupN.3\n",
        "\n",
        "# now try some different cluster groups\n",
        "# we are choosing no.3 this time\n",
        "\n",
        "#we are following our previous steps\n",
        "\n",
        "gabiitextools_class3=gabiitextools_class4.loc[gabiitextools_class4['cl']==3]\n",
        "gabiitextools_class3noarea=gabiitextools_class4.loc[gabiitextools_class4['cl2']==3]\n",
        "gabiitextools_k7_class3=gabiitextools_class4.loc[gabiitextools_class4['cl3']==3]\n",
        "gabiitextools_k7_class3noarea=gabiitextools_class4.loc[gabiitextools_class4['cl4']==3]\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2,nrows=2,figsize=(15, 10))\n",
        "gabiitextools_class0.plot(ax=axes[0,0]).axis('equal')\n",
        "axes[0,0].set_title('cl - 5 clusters - area')\n",
        "gabiitextools_class0noarea.plot(ax=axes[0,1]).axis('equal')\n",
        "axes[1,0].set_title('cl2 - 5 clusters - no area')\n",
        "gabiitextools_k7_class0.plot(ax=axes[1,0]).axis('equal')\n",
        "axes[0,1].set_title('cl3 - 7 clusters - area')\n",
        "gabiitextools_k7_class0noarea.plot(ax=axes[1,1]).axis('equal')\n",
        "axes[1,1].set_title('cl - 7 clusters - no area')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOd4CQew-BRw",
        "colab_type": "text"
      },
      "source": [
        "Cluster analysis is an important statistical technique. While not the main focus of this course, it's worth learning more about it. \n",
        "\n",
        "We have used K-Means clustering because it presents the advantage to be fast, easy to understand and fairly robust and efficient; however, it has disadvantages too, such as the number of groups/classes selection (and as you have seen which isn’t always inconsequential; ideally, you'd want the algorithm to figure this out). Furthermore, K-Means clustering cannot deal with non-linear dataset and if  there are  highly overlapping data then k-means will not be able to resolve the differentiation between clusters. Best results can be achieved when datasets are distinct or well separated from each other.\n",
        "\n",
        " **Reminder: linear versus non-linear datapoints:**<br>\n",
        "<img src=\"http://www.statistics4u.com/fundstat_eng/img/hl_linnonlin_classif.png\n",
        "\" width=\"600\"/> </div>\n",
        "<br>\n",
        "\n",
        "K-means is not the only clustering technique. I encourage you to do some independent reading on this technique. You may want to have a look at [this](https://scikit-learn.org/stable/modules/clustering.html#clustering) to start with. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLtpOqX0k4f",
        "colab_type": "text"
      },
      "source": [
        "####**Data interpretation: Using association and relationships**####\n",
        "\n",
        "Although we can see differences, it is difficult to:\n",
        "* see them all\n",
        "* quantify them\n",
        "* express them\n",
        "Let's borrow to Statitics to do this for us. <br>\n",
        "In ##codecell_SpatialPatterns_ImportUrLibraries, you have imported [seaborn library](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to do just that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhAjsR6Y4Fj0",
        "colab_type": "text"
      },
      "source": [
        "#####**Reminder**#####\n",
        "\n",
        "This may take a full practical lab to introduce statitistical thinking behind correlations, which are statistical associations between two variables. But do not imply causation!<br>\n",
        "<br>\n",
        "* Correlation tests describe the statistical relationship between two different variables, in contrast to comparison tests which evaluate the differences between variables (e.g. t-test, sign test, Mann-Whitney). \n",
        "* Another important characteristic of the correlation statistics is that they do not only identify relationships but they characterise both the strength and the form of these relationships. \n",
        "* The robustness of the correlation calculations is, again, evaluated in terms of statistical significance using the p-value. \n",
        "\n",
        "The most frequently used Parametric correlation statistic is the **Pearson’s correlation r**. Pearson’s r tests if there is a linear relationship between two variables, and can only range between -1 and 1, where -1 indicates perfect negative **linear** correlation and +1 indicates the perfect positive linear correlation (see Figure below). \n",
        "<img src=\"https://github.com/Francoz-Charlotte/Spatial_teaching_CFediting/blob/master/LinearCorre.png?raw=1\" width=\"800\"/> </div>\n",
        " **Examples of Pearson's correlation coefficient (r) for different degrees of linearity**\n",
        "\n",
        "Note that besides the sign of the correlation (positive or negative) the key term here is the term linear. We can only argue about linear relationships when we apply **parametric** correlation. A general **Non-Parametric** correlation statistic is the Spearman’s ρ (rho) correlation.\n",
        "<img src=\"https://github.com/Francoz-Charlotte/Spatial_teaching_CFediting/blob/master/SpearCorre.png?raw=1\" width=\"800\"/> </div>\n",
        "**Examples of Spearman's correlation coefficient (r) for different degrees of monotonic and non-monotonic relationships**\n",
        "\n",
        "As you can read, a fundamental concept is hidden behind this type of statitistic tests, and, in fact behind all statistics, which is parametric vs. nonparametric... you can start finding more about it [here](https://blog.minitab.com/blog/applying-statistics-in-quality-projects/a-correspondence-table-for-non-parametric-and-parametric-tests).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kkk4vCFv-7ur",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_EffectofClusterNumber\n",
        "\n",
        "# Do 7 clusters as oppossed to 5 result in more correlation?\n",
        "# we are simply using sns.pairplot function to estimate relationships between the groups\n",
        "\n",
        "sns.pairplot(gabiitextools_k7_class0.drop(['OBJECTID','DESCRIPTIO','Shape_Length','Shape_Area','SU','geometry','cl','cl2','cl3','cl4'], axis=1), kind=\"reg\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YZe1WBJ_-7M-",
        "colab": {}
      },
      "source": [
        "##codecell_SpatialPatterns_EffectofClusterNumber\n",
        "\n",
        "# Are some cluster classes/groups more correlated than others?\n",
        "sns.pairplot(gabiitextools_class0.drop(['OBJECTID','DESCRIPTIO','Shape_Length','Shape_Area','SU','geometry','cl','cl2','cl3','cl4'], axis=1), kind=\"reg\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiQg_M8G-BSd",
        "colab_type": "text"
      },
      "source": [
        "### **That concludes this tutorial**###\n",
        "\n",
        "Hopefully you have:\n",
        "* started thinking (and perhaps are a bit confused) about how spatial patterns of different types of finds are created, and how we can interpret them when studying data from an excavation.\n",
        "* learned to combine spatial data and descriptive tables. \n",
        "* learned to use some basic clustering tools, and reinforced your knowledge about how to make charts and maps. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNeDT5daQxgm",
        "colab_type": "text"
      },
      "source": [
        "#**LexiCode**\n",
        "To re-use the codes - you will need to first load their respective libraries.  So far, you have used ...:\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "> **libraries** | | |\n",
        ">--- |--- | --- | \n",
        ">folium | numpy  | \n",
        ">branca| rtree | \n",
        ">pandas| osmnx| \n",
        ">geopandas| requests | \n",
        ">seaborn | fiona| \n",
        ">matplotlib.pyplot | ipywidgets|\n",
        "> pysal |seaborn |\n",
        "> \n",
        "<br>\n",
        "\n",
        " **plugins**| |\n",
        "--- |--- |\n",
        "HeatMapWithTime\n",
        "HeatMap\n",
        "MeasureControl\n",
        "PrepareUrBasemaps_CreateLayers from [folium.plugins]\n",
        "cluster (from sklearn)\n",
        "\n",
        "<br>\n",
        "\n",
        "your lexicode is non-exhaustive, keep expanding, find your own 'best way' to reuse these code/scripts...\n",
        "\n",
        "<br>\n",
        "\n",
        ">Lexicode_MakingaBasicMap | Lexicode_Webmaps&Distributions |Lexicode_StreetGridOrientations | Lexicode_SpatialPatterns\n",
        ">--- | --- | ---|---|\n",
        ">\t==   () [] | pd.concat() | { } *subselection from list*|\n",
        ">.head_csv() | .dtype() | ox.gdf_from_places()|requests.get()|requests.get()\n",
        ">.read_csv() | astype() | ox.plot_shape()|request.content()\n",
        ">mean()  | fillna()|network_type= ''|.bytes()\n",
        ">folium.Map | def return |ox.add_edge_bearings(ox.get_undirected())|gpd.GeoDataFrame.from_features()\n",
        ">range() | .apply(lambda x:*function*,axis=) |count_and_merge()|Set()\n",
        ">len() | pd.merge() |np.arrange()|pd.value_counts() \n",
        ">iloc[]| how= , left_index= ,left_index= |np.histogram()|.merge()\n",
        ">.value_counts()| gpd.GeoDataFrame()| ax.set_theta_location()|.sort_values\n",
        ">if =:| geometry=gpd.points_from_xy |ax.set_ylim()|cluster.KMeans()\n",
        ">elif =: |print() |ax.set_title()|.fit()\n",
        ">else =:| .isin()|ax.set_yticks()|.drop()\n",
        ">folium.Marker()| classic.plot()|ax.set_xlabels() & ax.set_yticklabels|.assign()\n",
        ">folium.Icon()| generateBaseMap()|plt.subplots()|plt.show()\n",
        ">folium.Circle| .groupby(['', ''])|.dropna()|.set_title\n",
        ">popup= | .reset_index() |polar_plot()|sns.pairplot()\n",
        ">radius= |  max_zoom= |pd.Series()|\n",
        ">.values.tolist() |folium.TileLayer()|np.pi|\n",
        "> .add_to()| plugins.DualMap(location= , tiles= , zoom_start= )|\n",
        ">  | \n",
        "\n"
      ]
    }
  ]
}